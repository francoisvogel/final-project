{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T17:16:00.986427Z",
     "iopub.status.busy": "2025-10-05T17:16:00.982496Z",
     "iopub.status.idle": "2025-10-05T17:16:01.007118Z",
     "shell.execute_reply": "2025-10-05T17:16:01.005641Z",
     "shell.execute_reply.started": "2025-10-05T17:16:00.986320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/porto-seguro-safe-driver-prediction\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5511 Final Project — Porto Seguro Safe Driver (Tabular DL)\n",
    "\n",
    "**Problem.** Predict whether a customer will file an auto-insurance claim in the next year. Binary, **heavily imbalanced**.\n",
    "\n",
    "**Data & provenance.** Kaggle: *Porto Seguro Safe Driver Prediction* (`train.csv`). Features are anonymized: `*_num`, `*_bin`, `*_cat`. Missing values often encoded as `-1`. I use the public training split only. License/terms as on Kaggle.\n",
    "\n",
    "**Goal & metrics.** Optimize discrimination and ranking under class imbalance. I report **ROC-AUC**, **PR-AUC** (primary), Brier score (calibration), and confusion matrix at the **F1-optimal threshold** on a stratified hold-out set.\n",
    "\n",
    "**EDA**\n",
    "- Positive rate: ~3–4%.  \n",
    "- Missingness: several columns encode missing as `-1` → I add **missing indicators** per numeric feature.  \n",
    "- Numerics show skew; I **median-impute** and **standardize**.  \n",
    "- Categoricals: high-cardinality in places; I **label-encode** with train-only vocab + **unknown** bucket for unseen.\n",
    "\n",
    "**Methods.**\n",
    "- **Baseline:** XGBoost (`gpu_hist` when available) with `scale_pos_weight`.\n",
    "- **Deep model:** MLP for tabular with **categorical embeddings** + numeric block, dropout, BCE with `pos_weight` (and a focal-loss ablation). Early stopping on **PR-AUC**.  \n",
    "- **Ablations:** (1) BCE vs Focal; (2) with vs without categorical embeddings; (3) class-balanced sampler on/off.  \n",
    "- **Calibration:** reliability plot + Brier score.\n",
    "\n",
    "**Validation.** Single stratified **80/20** split, `seed=42`. No time component in features, so plain split is acceptable. All preprocessing fit on train only.\n",
    "\n",
    "**Repro.**\n",
    "- Environment: Kaggle Notebook (CPU/GPU).  \n",
    "- Data path: `/kaggle/input/porto-seguro-safe-driver-prediction/train.csv`.  \n",
    "- Run cells top-to-bottom. Figures stored under `/kaggle/working/reports/figures/`. Summary CSV at `/kaggle/working/results_summary.csv`.  \n",
    "- Random seeds fixed where supported.\n",
    "\n",
    "**Notes / limitations.**\n",
    "- Features are anonymized engineered signals; external covariates were not added.\n",
    "- On tabular data, tree ensembles are strong baselines; I include both and discuss trade-offs.\n",
    "\n",
    "**Academic honesty.** This is my own work. I used public documentation for library usage; all sources are cited where relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T18:52:34.301793Z",
     "iopub.status.busy": "2025-10-05T18:52:34.301047Z",
     "iopub.status.idle": "2025-10-05T18:55:25.982704Z",
     "shell.execute_reply": "2025-10-05T18:55:25.981280Z",
     "shell.execute_reply.started": "2025-10-05T18:52:34.301696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] loss=0.011848 ROC-AUC=0.62250 PR-AUC=0.06066\n",
      "[Epoch 02] loss=0.011151 ROC-AUC=0.62456 PR-AUC=0.06276\n",
      "[Epoch 03] loss=0.011121 ROC-AUC=0.62662 PR-AUC=0.06328\n",
      "[Epoch 04] loss=0.011118 ROC-AUC=0.62600 PR-AUC=0.06359\n",
      "[Epoch 05] loss=0.011080 ROC-AUC=0.62470 PR-AUC=0.06217\n",
      "[Epoch 06] loss=0.011084 ROC-AUC=0.62477 PR-AUC=0.06343\n",
      "[Epoch 07] loss=0.011057 ROC-AUC=0.62210 PR-AUC=0.06289\n",
      "[Epoch 08] loss=0.011048 ROC-AUC=0.62141 PR-AUC=0.06247\n",
      "[Epoch 09] loss=0.011009 ROC-AUC=0.62559 PR-AUC=0.06322\n",
      "[Epoch 10] loss=0.011001 ROC-AUC=0.61997 PR-AUC=0.06187\n",
      "Early stopping.\n",
      "[MLP-Emb] ROC-AUC: 0.61997 PR-AUC: 0.06187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as nnF\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import numpy as np\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    class TabDataset(Dataset):\n",
    "        def __init__(self, df, y, num_cols_names, cat_cols_names):\n",
    "            self.num = torch.tensor(df[num_cols_names].values, dtype=torch.float32)\n",
    "            self.cat = torch.tensor(df[cat_cols_names].values, dtype=torch.long) if len(cat_cols_names) else None\n",
    "            self.y   = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "        def __len__(self): return len(self.num)\n",
    "        def __getitem__(self, i):\n",
    "            if self.y is None:\n",
    "                return self.num[i], (self.cat[i] if self.cat is not None else None)\n",
    "            return self.num[i], (self.cat[i] if self.cat is not None else None), self.y[i]\n",
    "\n",
    "    class TabMLP(nn.Module):\n",
    "        def __init__(self, n_num, cat_cardinalities, d_emb=32, hidden=(256,128), dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.has_cat = len(cat_cardinalities) > 0\n",
    "            if self.has_cat:\n",
    "                self.embs = nn.ModuleList([nn.Embedding(card, d_emb) for card in cat_cardinalities])\n",
    "                cat_dim = d_emb * len(cat_cardinalities)\n",
    "            else:\n",
    "                self.embs = nn.ModuleList()\n",
    "                cat_dim = 0\n",
    "            in_dim = n_num + cat_dim\n",
    "            layers = []\n",
    "            last = in_dim\n",
    "            for h in hidden:\n",
    "                layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "                last = h\n",
    "            layers += [nn.Linear(last, 1)]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, num, cat=None):\n",
    "            if self.has_cat and cat is not None:\n",
    "                embs = [emb(cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "                cat_feat = torch.cat(embs, dim=1)\n",
    "                x = torch.cat([num, cat_feat], dim=1)\n",
    "            else:\n",
    "                x = num\n",
    "            return self.net(x).squeeze(1)  # logits\n",
    "\n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "            super().__init__()\n",
    "            self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n",
    "        def forward(self, logits, targets):\n",
    "            # targets: 0/1 long or float; logits: raw\n",
    "            bce = nnF.binary_cross_entropy_with_logits(logits, targets.float(), reduction=\"none\")\n",
    "            p   = torch.sigmoid(logits)\n",
    "            pt  = p*targets + (1-p)*(1-targets)\n",
    "            loss = self.alpha * (1-pt).pow(self.gamma) * bce\n",
    "            return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "    def train_mlp(\n",
    "        Xtr, ytr, Xva, yva,\n",
    "        num_cols_names, cat_cols_names, cat_cardinalities,\n",
    "        epochs=50, batch=2048, lr=3e-4, d_emb=32, hidden=(256,128), dropout=0.2,\n",
    "        use_focal=True, class_weight=True, early_stop_pat=6\n",
    "    ):\n",
    "        pin = torch.cuda.is_available()\n",
    "        tr_ds = TabDataset(Xtr, ytr, num_cols_names, cat_cols_names)\n",
    "        va_ds = TabDataset(Xva, yva, num_cols_names, cat_cols_names)\n",
    "        tr_ld = DataLoader(tr_ds, batch_size=batch, shuffle=True,  num_workers=2, pin_memory=pin)\n",
    "        va_ld = DataLoader(va_ds, batch_size=batch*2, shuffle=False, num_workers=2, pin_memory=pin)\n",
    "\n",
    "        model = TabMLP(len(num_cols_names), cat_cardinalities, d_emb, hidden, dropout).to(DEVICE)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "        if use_focal:\n",
    "            criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        else:\n",
    "            if class_weight:\n",
    "                pos_w = (ytr==0).sum() / max(1,(ytr==1).sum())\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=DEVICE))\n",
    "            else:\n",
    "                criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        best_pr = -1.0\n",
    "        best_state = None\n",
    "        patience = 0\n",
    "\n",
    "        from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            model.train()\n",
    "            total = 0.0\n",
    "            for batch in tr_ld:\n",
    "                num, cat, yy = batch\n",
    "                num = num.to(DEVICE, non_blocking=True)\n",
    "                cat = (cat.to(DEVICE, non_blocking=True) if cat is not None else None)\n",
    "                yy  = yy.to(DEVICE, non_blocking=True)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                logits = model(num, cat)\n",
    "                loss = criterion(logits, yy.float())\n",
    "                loss.backward(); opt.step()\n",
    "                total += loss.item() * yy.size(0)\n",
    "\n",
    "            # eval\n",
    "            model.eval()\n",
    "            preds, ys = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in va_ld:\n",
    "                    num, cat, yy = batch\n",
    "                    num = num.to(DEVICE, non_blocking=True)\n",
    "                    cat = (cat.to(DEVICE, non_blocking=True) if cat is not None else None)\n",
    "                    logits = model(num, cat)\n",
    "                    preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                    ys.append(yy.numpy())\n",
    "            preds = np.concatenate(preds); ys = np.concatenate(ys)\n",
    "            roc = roc_auc_score(ys, preds)\n",
    "            pr  = average_precision_score(ys, preds)\n",
    "            print(f\"[Epoch {ep:02d}] loss={total/len(tr_ds):.6f} ROC-AUC={roc:.5f} PR-AUC={pr:.5f}\")\n",
    "\n",
    "            if pr > best_pr + 1e-5:\n",
    "                best_pr = pr\n",
    "                best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= early_stop_pat:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "        return model\n",
    "\n",
    "    cat_cardinalities = [int(X_tr[c].max()) + 1 for c in cat_cols]\n",
    "\n",
    "    mlp = train_mlp(\n",
    "        X_tr, y_tr, X_te, y_te,\n",
    "        num_cols_names=num_cols,\n",
    "        cat_cols_names=cat_cols,\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        epochs=50, batch=2048, lr=3e-4, d_emb=32, hidden=(256,128), dropout=0.2,\n",
    "        use_focal=True, class_weight=True, early_stop_pat=6\n",
    "    )\n",
    "\n",
    "    # Inference on validation\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    va_ds = TabDataset(X_te, None, num_cols, cat_cols)\n",
    "    va_ld = DataLoader(va_ds, batch_size=4096, shuffle=False, pin_memory=torch.cuda.is_available())\n",
    "    mlp.eval(); preds=[]\n",
    "    with torch.no_grad():\n",
    "        for num, cat in va_ld:\n",
    "            num = num.to(DEVICE, non_blocking=True)\n",
    "            cat = (cat.to(DEVICE, non_blocking=True) if cat is not None else None)\n",
    "            preds.append(torch.sigmoid(mlp(num, cat)).cpu().numpy())\n",
    "    proba_mlp = np.concatenate(preds)\n",
    "\n",
    "    print(\"[MLP-Emb] ROC-AUC:\", roc_auc_score(y_te, proba_mlp).round(5),\n",
    "        \"PR-AUC:\", average_precision_score(y_te, proba_mlp).round(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 874852,
     "sourceId": 7082,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
