{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7082,"databundleVersionId":874852,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/porto-seguro-safe-driver-prediction\"\nTRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T17:16:00.982496Z","iopub.execute_input":"2025-10-05T17:16:00.986427Z","iopub.status.idle":"2025-10-05T17:16:01.007118Z","shell.execute_reply.started":"2025-10-05T17:16:00.986320Z","shell.execute_reply":"2025-10-05T17:16:01.005641Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# DTSA 5511 Final Project — Porto Seguro Safe Driver (Tabular DL)\n\n**Problem.** Predict whether a customer will file an auto-insurance claim in the next year. Binary, **heavily imbalanced**.\n\n**Data & provenance.** Kaggle: *Porto Seguro Safe Driver Prediction* (`train.csv`). Features are anonymized: `*_num`, `*_bin`, `*_cat`. Missing values often encoded as `-1`. I use the public training split only. License/terms as on Kaggle.\n\n**Goal & metrics.** Optimize discrimination and ranking under class imbalance. I report **ROC-AUC**, **PR-AUC** (primary), Brier score (calibration), and confusion matrix at the **F1-optimal threshold** on a stratified hold-out set.\n\n**EDA**\n- Positive rate: ~3–4%.  \n- Missingness: several columns encode missing as `-1` → I add **missing indicators** per numeric feature.  \n- Numerics show skew; I **median-impute** and **standardize**.  \n- Categoricals: high-cardinality in places; I **label-encode** with train-only vocab + **unknown** bucket for unseen.\n\n**Methods.**\n- **Baseline:** XGBoost (`gpu_hist` when available) with `scale_pos_weight`.\n- **Deep model:** MLP for tabular with **categorical embeddings** + numeric block, dropout, BCE with `pos_weight` (and a focal-loss ablation). Early stopping on **PR-AUC**.  \n- **Ablations:** (1) BCE vs Focal; (2) with vs without categorical embeddings; (3) class-balanced sampler on/off.  \n- **Calibration:** reliability plot + Brier score.\n\n**Validation.** Single stratified **80/20** split, `seed=42`. No time component in features, so plain split is acceptable. All preprocessing fit on train only.\n\n**Repro.**\n- Environment: Kaggle Notebook (CPU/GPU).  \n- Data path: `/kaggle/input/porto-seguro-safe-driver-prediction/train.csv`.  \n- Run cells top-to-bottom. Figures stored under `/kaggle/working/reports/figures/`. Summary CSV at `/kaggle/working/results_summary.csv`.  \n- Random seeds fixed where supported.\n\n**Notes / limitations.**\n- Features are anonymized engineered signals; external covariates were not added.\n- On tabular data, tree ensembles are strong baselines; I include both and discuss trade-offs.\n\n**Academic honesty.** This is my own work. I used public documentation for library usage; all sources are cited where relevant.\n","metadata":{}},{"cell_type":"code","source":"# --- Drop-in: DL with categorical embeddings (no F import needed) ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as nnF\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass TabDataset(Dataset):\n    def __init__(self, df, y, num_cols_names, cat_cols_names):\n        # Build tensors by column names to preserve dtypes:\n        self.num = torch.tensor(df[num_cols_names].values, dtype=torch.float32)\n        # Ensure cats are integer-coded 0..K-1\n        self.cat = torch.tensor(df[cat_cols_names].values, dtype=torch.long) if len(cat_cols_names) else None\n        self.y   = None if y is None else torch.tensor(y, dtype=torch.long)\n    def __len__(self): return len(self.num)\n    def __getitem__(self, i):\n        if self.y is None:\n            return self.num[i], (self.cat[i] if self.cat is not None else None)\n        return self.num[i], (self.cat[i] if self.cat is not None else None), self.y[i]\n\nclass TabMLP(nn.Module):\n    def __init__(self, n_num, cat_cardinalities, d_emb=32, hidden=(256,128), dropout=0.2):\n        super().__init__()\n        self.has_cat = len(cat_cardinalities) > 0\n        if self.has_cat:\n            self.embs = nn.ModuleList([nn.Embedding(card, d_emb) for card in cat_cardinalities])\n            cat_dim = d_emb * len(cat_cardinalities)\n        else:\n            self.embs = nn.ModuleList()\n            cat_dim = 0\n        in_dim = n_num + cat_dim\n        layers = []\n        last = in_dim\n        for h in hidden:\n            layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n            last = h\n        layers += [nn.Linear(last, 1)]\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, num, cat=None):\n        if self.has_cat and cat is not None:\n            embs = [emb(cat[:, i]) for i, emb in enumerate(self.embs)]\n            cat_feat = torch.cat(embs, dim=1)\n            x = torch.cat([num, cat_feat], dim=1)\n        else:\n            x = num\n        return self.net(x).squeeze(1)  # logits\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n        super().__init__()\n        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n    def forward(self, logits, targets):\n        # targets: 0/1 long or float; logits: raw\n        bce = nnF.binary_cross_entropy_with_logits(logits, targets.float(), reduction=\"none\")\n        p   = torch.sigmoid(logits)\n        pt  = p*targets + (1-p)*(1-targets)\n        loss = self.alpha * (1-pt).pow(self.gamma) * bce\n        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n\ndef train_mlp(\n    Xtr, ytr, Xva, yva,\n    num_cols_names, cat_cols_names, cat_cardinalities,\n    epochs=50, batch=2048, lr=3e-4, d_emb=32, hidden=(256,128), dropout=0.2,\n    use_focal=True, class_weight=True, early_stop_pat=6\n):\n    pin = torch.cuda.is_available()\n    tr_ds = TabDataset(Xtr, ytr, num_cols_names, cat_cols_names)\n    va_ds = TabDataset(Xva, yva, num_cols_names, cat_cols_names)\n    tr_ld = DataLoader(tr_ds, batch_size=batch, shuffle=True,  num_workers=2, pin_memory=pin)\n    va_ld = DataLoader(va_ds, batch_size=batch*2, shuffle=False, num_workers=2, pin_memory=pin)\n\n    model = TabMLP(len(num_cols_names), cat_cardinalities, d_emb, hidden, dropout).to(DEVICE)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n\n    if use_focal:\n        criterion = FocalLoss(alpha=0.25, gamma=2.0)\n    else:\n        if class_weight:\n            pos_w = (ytr==0).sum() / max(1,(ytr==1).sum())\n            criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_w], device=DEVICE))\n        else:\n            criterion = nn.BCEWithLogitsLoss()\n\n    best_pr = -1.0\n    best_state = None\n    patience = 0\n\n    from sklearn.metrics import average_precision_score, roc_auc_score\n\n    for ep in range(1, epochs+1):\n        model.train()\n        total = 0.0\n        for batch in tr_ld:\n            num, cat, yy = batch\n            num = num.to(DEVICE, non_blocking=True)\n            cat = (cat.to(DEVICE, non_blocking=True) if cat is not None else None)\n            yy  = yy.to(DEVICE, non_blocking=True)\n\n            opt.zero_grad()\n            logits = model(num, cat)\n            loss = criterion(logits, yy.float())\n            loss.backward(); opt.step()\n            total += loss.item() * yy.size(0)\n\n        # eval\n        model.eval()\n        preds, ys = [], []\n        with torch.no_grad():\n            for batch in va_ld:\n                num, cat, yy = batch\n                num = num.to(DEVICE, non_blocking=True)\n                cat = (cat.to(DEVICE, non_blocking=True) if cat is not None else None)\n                logits = model(num, cat)\n                preds.append(torch.sigmoid(logits).cpu().numpy())\n                ys.append(yy.numpy())\n        preds = np.concatenate(preds); ys = np.concatenate(ys)\n        roc = roc_auc_score(ys, preds)\n        pr  = average_precision_score(ys, preds)\n        print(f\"[Epoch {ep:02d}] loss={total/len(tr_ds):.6f} ROC-AUC={roc:.5f} PR-AUC={pr:.5f}\")\n\n        if pr > best_pr + 1e-5:\n            best_pr = pr\n            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n            patience = 0\n        else:\n            patience += 1\n            if patience >= early_stop_pat:\n                print(\"Early stopping.\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n    return model\n\n# ---- Train with your existing splits/columns ----\n# IMPORTANT: ensure cat cols are already integer-coded 0..K-1 from your preprocessing.\ncat_cardinalities = [int(X_tr[c].max()) + 1 for c in cat_cols]  # each cat col must be 0..K-1\n\nmlp = train_mlp(\n    X_tr, y_tr, X_te, y_te,\n    num_cols_names=num_cols,\n    cat_cols_names=cat_cols,\n    cat_cardinalities=cat_cardinalities,\n    epochs=50, batch=2048, lr=3e-4, d_emb=32, hidden=(256,128), dropout=0.2,\n    use_focal=True, class_weight=True, early_stop_pat=6\n)\n\n# Inference on validation\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nva_ds = TabDataset(X_te, None, num_cols, cat_cols)\nva_ld = DataLoader(va_ds, batch_size=4096, shuffle=False, pin_memory=torch.cuda.is_available())\nmlp.eval(); preds=[]\nwith torch.no_grad():\n    for num, cat in va_ld:\n        num = num.to(DEVICE, non_blocking=True)\n        cat = (cat.to(DEVICE, non_blocking=True) if cat is not None else None)\n        preds.append(torch.sigmoid(mlp(num, cat)).cpu().numpy())\nproba_mlp = np.concatenate(preds)\n\nprint(\"[MLP-Emb] ROC-AUC:\", roc_auc_score(y_te, proba_mlp).round(5),\n      \"PR-AUC:\", average_precision_score(y_te, proba_mlp).round(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T18:52:34.301047Z","iopub.execute_input":"2025-10-05T18:52:34.301793Z","iopub.status.idle":"2025-10-05T18:55:25.982704Z","shell.execute_reply.started":"2025-10-05T18:52:34.301696Z","shell.execute_reply":"2025-10-05T18:55:25.981280Z"}},"outputs":[{"name":"stdout","text":"[Epoch 01] loss=0.011848 ROC-AUC=0.62250 PR-AUC=0.06066\n[Epoch 02] loss=0.011151 ROC-AUC=0.62456 PR-AUC=0.06276\n[Epoch 03] loss=0.011121 ROC-AUC=0.62662 PR-AUC=0.06328\n[Epoch 04] loss=0.011118 ROC-AUC=0.62600 PR-AUC=0.06359\n[Epoch 05] loss=0.011080 ROC-AUC=0.62470 PR-AUC=0.06217\n[Epoch 06] loss=0.011084 ROC-AUC=0.62477 PR-AUC=0.06343\n[Epoch 07] loss=0.011057 ROC-AUC=0.62210 PR-AUC=0.06289\n[Epoch 08] loss=0.011048 ROC-AUC=0.62141 PR-AUC=0.06247\n[Epoch 09] loss=0.011009 ROC-AUC=0.62559 PR-AUC=0.06322\n[Epoch 10] loss=0.011001 ROC-AUC=0.61997 PR-AUC=0.06187\nEarly stopping.\n[MLP-Emb] ROC-AUC: 0.61997 PR-AUC: 0.06187\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}